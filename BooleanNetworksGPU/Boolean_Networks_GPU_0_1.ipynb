{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_xWzNL3Tpe6"
      },
      "source": [
        "# Simulação de Redes Booleanas\n",
        "\n",
        "O presente *notebook* tem como objetivo explorar a simulação síncrona de redes booleanas, que são modeladas com base em sistemas biológicos e frequentemente servem de base para experimentos *in silico* que visam entender a dinâmica da interação entre genes, moléculas ou outros componentes que façam parte de sistemas biológicos. \n",
        "\n",
        "Sabendo que simular $N$ possíveis estados é uma tarefa de complexidade exponencial, visto que temos $2^{N}$ possíveis estados, faz-se necessário utilizar estratégias diminuam tempo de execução de execução. Como o principal objetivo das simulações é obter um conjunto de probabilidades associada a cada agente biológico estudado e a exploração dos estados da rede é independente de outros estados que não o anterior, este problema se torna candidato a otimização utilizando computação paralela."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYgLQflo-MX7"
      },
      "source": [
        "# Dependencies\n",
        "\n",
        "Plugins necessários para execução de código em CUDA com a *cell magic* `%%gpu`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5aHy4BNZ2RR",
        "outputId": "2c39b823-edfd-483b-a4dc-b1d81459da6d"
      },
      "source": [
        "!pip install git+git://github.com/canesche/nvcc4jupyter.git\n",
        "!git clone https://github.com/canesche/nvcc4jupyter\n",
        "!git clone https://github.com/lucas-t-reis/research.git\n",
        "%load_ext nvcc_plugin"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+git://github.com/canesche/nvcc4jupyter.git\n",
            "  Cloning git://github.com/canesche/nvcc4jupyter.git to /tmp/pip-req-build-23t_mii0\n",
            "  Running command git clone -q git://github.com/canesche/nvcc4jupyter.git /tmp/pip-req-build-23t_mii0\n",
            "Building wheels for collected packages: ColabPlugin\n",
            "  Building wheel for ColabPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ColabPlugin: filename=ColabPlugin-blind-cp37-none-any.whl size=12717 sha256=edfc012e82d34954b2d0320c8b8d321b76175442859d8c8a4d128e5aab73251e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-05wfv2gu/wheels/87/ae/09/21c6e192181a0472e20ddd1d5212e2cbb113f00ebe14330d0d\n",
            "Successfully built ColabPlugin\n",
            "Installing collected packages: ColabPlugin\n",
            "Successfully installed ColabPlugin-blind\n",
            "Cloning into 'nvcc4jupyter'...\n",
            "remote: Enumerating objects: 362, done.\u001b[K\n",
            "remote: Counting objects: 100% (362/362), done.\u001b[K\n",
            "remote: Compressing objects: 100% (271/271), done.\u001b[K\n",
            "remote: Total 1147 (delta 100), reused 328 (delta 74), pack-reused 785\u001b[K\n",
            "Receiving objects: 100% (1147/1147), 35.71 MiB | 19.57 MiB/s, done.\n",
            "Resolving deltas: 100% (554/554), done.\n",
            "Cloning into 'research'...\n",
            "remote: Enumerating objects: 106, done.\u001b[K\n",
            "remote: Counting objects: 100% (106/106), done.\u001b[K\n",
            "remote: Compressing objects: 100% (103/103), done.\u001b[K\n",
            "remote: Total 357 (delta 5), reused 91 (delta 1), pack-reused 251\u001b[K\n",
            "Receiving objects: 100% (357/357), 2.18 MiB | 25.72 MiB/s, done.\n",
            "Resolving deltas: 100% (44/44), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGuG03xQVl6i"
      },
      "source": [
        "GPU utilizada nos experimentos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvozSy1EB8Td",
        "outputId": "14ad9c82-4bc6-402b-f2a8-6708c7f05748"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Mar 25 21:43:11 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.56       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   68C    P8    11W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfBN3nScVw6X"
      },
      "source": [
        "Neste arquivo de cabeçalho, se encontram:\n",
        "\n",
        "- Macro de verificação de erros para GPU;\n",
        "- Encapsulamento da implementação de um Cronômetro para CPU e para GPU;\n",
        "- Função auxiliar para imprimir os tempos de cada execução em um arquivo `.csv`;\n",
        "- Função auxiliar para imprimir o histograma obtido após as simulações;"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-AAwzULc6xnW",
        "outputId": "e0d02ae4-2502-428d-e5fb-8b38e746a450"
      },
      "source": [
        "%%writefile profiler.h\n",
        "#ifndef PROFILER_H\n",
        "#define PROFILER_H\n",
        "\n",
        "#include<chrono>\n",
        "#include<fstream>\n",
        "\n",
        "// Careful for overflow above 9. Would require changing int to long long\n",
        "#define MAX_POWER10 9\n",
        "\n",
        "//Macro for checking cuda errors following a cuda launch or api call\n",
        "#define cudaCheckError() {                                                      \\\n",
        " cudaError_t e=cudaGetLastError();                                              \\\n",
        " if(e!=cudaSuccess) {                                                           \\\n",
        "   printf(\"Cuda failure %s:%d: '%s'\\n\",__FILE__,__LINE__,cudaGetErrorString(e));\\\n",
        "   exit(0);                                                                     \\\n",
        " }                                                                              \\\n",
        "}\n",
        "\n",
        "class CPU_Stopwatch {\n",
        "    \n",
        "    typedef std::chrono::steady_clock clock;\n",
        "    \n",
        "    private:\n",
        "      std::chrono::time_point<clock> start;\n",
        "\n",
        "    public:\n",
        "      void reset() { start = clock::now(); }\n",
        "      CPU_Stopwatch() { reset(); }\n",
        "      auto stop() { \n",
        "          std::chrono::duration<float> elapsed_time = clock::now() - start;\n",
        "          return elapsed_time.count() * 1000;\n",
        "      }\n",
        "};\n",
        "\n",
        "class GPU_Stopwatch {\n",
        "    \n",
        "    private:\n",
        "      cudaEvent_t start;\n",
        "      cudaEvent_t end;\n",
        "\n",
        "    public:\n",
        "      GPU_Stopwatch() {\n",
        "          \n",
        "          cudaEventCreate(&start);\n",
        "          cudaEventCreate(&end);\n",
        "          \n",
        "          cudaEventRecord(start);\n",
        "      }\n",
        "      float stop() {\n",
        "\n",
        "          cudaEventRecord(end);\n",
        "\n",
        "          cudaEventSynchronize(end);\n",
        "          float milliseconds = -1;\n",
        "          cudaEventElapsedTime(&milliseconds, start, end);\n",
        "\n",
        "          return milliseconds;\n",
        "\n",
        "      }\n",
        "    \n",
        "};\n",
        "\n",
        "void write_csv(const char * fileName, float profiling_times[], size_t n) {\n",
        "    \n",
        "    std::ofstream csv(fileName, std::ofstream::app);\n",
        "    for(int i=0; i<n; i++) {\n",
        "      if(i==n-1)\n",
        "        csv << profiling_times[i] << \"\\n\";\n",
        "      else\n",
        "        csv << profiling_times[i] << \",\";    \n",
        "    }    \n",
        "}\n",
        "void reduce_histogram(int *histogram, int *thread_histograms, size_t size, size_t n_genes) {\n",
        "\n",
        "    //#pragma unroll\n",
        "    for(int i=0; i<size; i++)\n",
        "        histogram[i%n_genes] += thread_histograms[i];\n",
        "}\n",
        "void print_histogram(int *histogram, size_t n) {\n",
        "    printf(\"Histogram [\");\n",
        "    for(int i=0; i<n; i++) {\n",
        "        if(i==n-1) printf(\"%.2lf]\\n\\n\", histogram[i]/(1000000000.0));\n",
        "        else printf(\"%.2lf,\", histogram[i]/(1000000000.0));\n",
        "    }\n",
        "}\n",
        "\n",
        "#endif"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing profiler.h\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCEWmiQocCNv"
      },
      "source": [
        "# Node activation estimation based on Simulation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwhBk90ODiT0"
      },
      "source": [
        "## Equation parser\n",
        "\n",
        "A simple python implementation to convert `.bnet` files into `c++` headers with a given boolean network configuration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMmJ3YFWD4b2",
        "outputId": "0b626e79-541b-41f9-83fd-9eecad3c7f82"
      },
      "source": [
        "%%writefile equation_to_code.py\n",
        "\n",
        "import random\n",
        "import sys\n",
        "import re\n",
        "\n",
        "# Removes non-alphanumeric characters from the String.\n",
        "def filter(t):\n",
        "    if t[0].isnumeric():\n",
        "        print(  'Gene names can\\'t start with numbers.' \n",
        "                'Try renaming the gene %s on all of its occurrences'\n",
        "                '\\nAborting' % t)\n",
        "        exit()\n",
        "    \n",
        "    t = re.sub('[^a-zA-Z0-9\\-\\_]', '', t)\n",
        "    return t.replace('-', '_')\n",
        "\n",
        "#file = 'research/boolean-networks/networks/40-100/Apoptosis network/Apoptosis network.txt'\n",
        "file = sys.argv[1]\n",
        "skip = ['\\n', '#']\n",
        "operators = {'AND':' && ', 'NOT':'!', 'OR':' || ', '(':'(', ')':')'}\n",
        "fixed_value = ['TRUE', 'FALSE', 'RANDOM']\n",
        "\n",
        "node_idx = {}\n",
        "fixed_node = {}\n",
        "\n",
        "i = 0\n",
        "# Assigning index to nodes\n",
        "with open(file) as input:\n",
        "    for line in input.readlines():\n",
        "        if line[0] in skip:\n",
        "            continue\n",
        "\n",
        "        node, equation = line.split('=')\n",
        "        \n",
        "        # Reading fixed states\n",
        "        if equation.strip() in fixed_value:\n",
        "            fixed_node[filter(node)] = equation.strip()\n",
        "            print('Fixed node %s with %s value' % (filter(node), fixed_node[filter(node)]))\n",
        "            continue\n",
        "\n",
        "        node_idx[filter(node)] = i\n",
        "        i += 1\n",
        "\n",
        "# Original nodes are needed for registers due to undeclared nodes within equations\n",
        "original_nodes = node_idx.copy()\n",
        "\n",
        "N = i\n",
        "code = ''\n",
        "with open(file) as input:\n",
        "    for line in input.readlines():\n",
        "        if line[0] in skip:\n",
        "            continue\n",
        "\n",
        "        node, equation = line.split('=')\n",
        "        \n",
        "        if filter(node) in fixed_node:\n",
        "            continue\n",
        "        \n",
        "        index = str( node_idx[filter(node)] )\n",
        "        code += '\\t\\ttemp_state[offset+'+index+'] = '\n",
        "        \n",
        "        for token in equation.split():\n",
        "            \n",
        "            # Fixed Nodes\n",
        "            if filter(token) in fixed_node:\n",
        "                value = fixed_node[filter(token)]\n",
        "                if value == 'RANDOM':\n",
        "                    value = 'TRUE' if random.randint(0,1) else 'FALSE'\n",
        "                    fixed_node[filter(token)] = value\n",
        "                    print('Setting RANDOM valued node %s to %s' % (token, fixed_node[filter(token)]))\n",
        "                \n",
        "                code += value.lower()\n",
        "\n",
        "            # Nodes\n",
        "            elif filter(token) in node_idx:\n",
        "                token_idx = str( node_idx[filter(token)] )\n",
        "                code += 'shared_state[offset+'+token_idx+']'\n",
        "            \n",
        "            # Nodes that are in RHS but were not present in the LHS of any line\n",
        "            elif token.strip() not in operators:\n",
        "                print('%s was not initialized with an equation. Assigning a random state' % token)\n",
        "                node_idx[filter(token)] = i\n",
        "                token_idx = str(i)\n",
        "                i += 1\n",
        "                \n",
        "                code += ' true ' if random.randint(0,1) else ' false '\n",
        "\n",
        "            # Operators\n",
        "            else:\n",
        "                code += operators[token.strip()]\n",
        "\n",
        "        code += ';\\n'\n",
        "\n",
        "\n",
        "\n",
        "# Printing the header files\n",
        "\n",
        "initializer = '''\n",
        "#ifndef INITIALIZER_H\n",
        "#define INITIALIZER_H\n",
        "#define N ''' + str(N) + '\\n' # i is greater tha N in some cases, due to uninitialized nodes\n",
        "for node in fixed_node:\n",
        "    index = str( node_idx[filter(node)] )\n",
        "    initializer += '\\tshared_state[offset+'+index+'] = '+fixed_node[filter(node)].lower()+';\\n'\n",
        "initializer += '#endif'\n",
        "\n",
        "equations = '''\n",
        "#ifndef EQUATIONS_H\n",
        "#define EQUATIONS_H\n",
        "''' + code + '#endif'\n",
        "\n",
        "registers = '''\n",
        "#ifndef REGISTERS_H\n",
        "#define REGISTERS_H\n",
        "'''\n",
        "for node in original_nodes:\n",
        "    registers += '\\tunsigned int ' + node + ' = 0;\\n'\n",
        "registers += '#endif\\n'\n",
        "\n",
        "registers_sum = '''\n",
        "#ifndef REGISTERS_SUM_H\n",
        "#define REGISTERS_SUM_H\n",
        "'''\n",
        "for node in original_nodes:\n",
        "    idx = str(node_idx[node])\n",
        "    registers_sum += '\\t\\t' + node + ' += shared_state[offset+'+idx+'];\\n'\n",
        "registers_sum += '#endif\\n'\n",
        "\n",
        "registers_reduction = '''\n",
        "#ifndef REGISTERS_REDUCTION_H\n",
        "#define REGISTERS_REDUCTION_H\n",
        "'''\n",
        "for node in original_nodes:\n",
        "    idx = str(node_idx[node])\n",
        "    registers_reduction += '\\tatomicAdd(&histogram['+idx+'],'+node+');\\n'\n",
        "registers_reduction += '#endif\\n'\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "with open('initializer.h', 'w') as output:\n",
        "    output.write(initializer)\n",
        "with open('equations.h', 'w') as output:\n",
        "    output.write(equations)\n",
        "with open('registers.h', 'w') as output:\n",
        "    output.write(registers)\n",
        "with open('registers_reduction.h', 'w') as output:\n",
        "    output.write(registers_reduction)\n",
        "with open('registers_sum.h', 'w') as output:\n",
        "    output.write(registers_sum)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing equation_to_code.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCC0nXnRpR8r"
      },
      "source": [
        "## GPU\n",
        "Em ambas as estratégias, a carga das iterações é dividida entre as *threads* da GPU. Além disso, cada uma delas possui uma cópia inicializada aleatoriamente da rede a ser analisada."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzyY5bkTFBOs"
      },
      "source": [
        "### Naive State Array\n",
        "\n",
        "Implementação que serve de Baseline para as técnicas de otimização em GPU.\n",
        "\n",
        "- Código do **kernel** é gerado para um grafo específico;\n",
        "\n",
        "- Utilização de *array* para representação dos estados e armazenamento do histograma;\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_6eIXztE5lZ",
        "outputId": "ae76296f-54b8-49e3-9da6-04c921085bb0"
      },
      "source": [
        "%%writefile GPU_naive.cu\n",
        "\n",
        "#include <string.h>\n",
        "#include <stdbool.h>\n",
        "#include <stdio.h>\n",
        "\n",
        "#include \"initializer.h\"\n",
        "#include \"profiler.h\"\n",
        "#include \"curand.h\"\n",
        "\n",
        "__global__\n",
        "void simulate(int iterations, bool *shared_state, bool *temp_state, int *histogram, size_t size) {\n",
        "\n",
        "    int tId = threadIdx.x + blockDim.x*blockIdx.x;\n",
        "    if(tId >= size-N) return;\n",
        "\n",
        "    // Displaces each thread to its respective portion of the array\n",
        "    int offset = tId*N;\n",
        "    \n",
        "    for(int i=0; i<iterations; i++) {\n",
        "\n",
        "        #include \"equations.h\"\n",
        "        \n",
        "        // Updating network for the next iteration\n",
        "        #pragma unroll\n",
        "        for(int j=0; j<N; j++) \n",
        "            shared_state[offset+j] = temp_state[offset+j];\n",
        "        \n",
        "        // Updating histogram\n",
        "        #pragma unroll\n",
        "        for(int j=0; j<N; j++)\n",
        "            histogram[offset+j] += shared_state[offset+j];\n",
        "\n",
        "    }\n",
        "\n",
        "}\n",
        "\n",
        "__global__\n",
        "void generate_starting_states(bool *states, float *rand, size_t size) {\n",
        "\n",
        "    int tId = threadIdx.x + blockIdx.x*blockDim.x;\n",
        "\n",
        "    // size-N accounts for the last tId iterating N times until the last position\n",
        "    if(tId >= size-N) return;\n",
        "\n",
        "    int offset = tId*N;\n",
        "    #pragma unroll\n",
        "    for(int i=0; i<N; i++)\n",
        "        states[offset+i] = rand[offset+i]>=0.5?true:false;\n",
        "\n",
        "}\n",
        "\n",
        "int main() {\n",
        "\t\n",
        "    \n",
        "    // Kernel variables\n",
        "    int threads = 128; \n",
        "    int blocks = (N+threads-1)/threads;\n",
        "    printf(\"threads:%d\\tblocks:%d\\n\", threads, blocks);\n",
        "\n",
        "    size_t n_bytes = sizeof(bool) * N * threads;\n",
        "    size_t hist_bytes = sizeof(int) * N * threads;\n",
        "    size_t size = N*threads;\n",
        "    \n",
        "    // Randomizing starting positions\n",
        "    float *d_rand;\n",
        "    cudaMalloc((void **) &d_rand, size*sizeof(float));\n",
        "    \n",
        "    curandGenerator_t gen;\n",
        "    curandCreateGenerator(&gen, CURAND_RNG_PSEUDO_DEFAULT);\n",
        "    curandSetPseudoRandomGeneratorSeed(gen, 1234ULL);\n",
        "    curandGenerateUniform(gen, d_rand, N*threads);\n",
        "    \n",
        "    // Device network and histogram variables\n",
        "    bool *d_state, *d_next_state;\n",
        "    int *d_histogram;\n",
        "    cudaMalloc((void**) &d_state, n_bytes);\n",
        "    cudaMalloc((void**) &d_next_state, n_bytes);\n",
        "    cudaMalloc((void**) &d_histogram, hist_bytes);\n",
        "\n",
        "    generate_starting_states<<<blocks,threads>>>(d_state, d_rand, size);\n",
        "\n",
        "    float elapsed_time[MAX_POWER10];\n",
        "    unsigned long long iterations = 10;\n",
        "    for(int t=0; t<MAX_POWER10; t++) {\n",
        "        \n",
        "        int split_workload = iterations / (blocks*threads) + 1;\n",
        "\n",
        "        //cudaMemset(d_state, 0, n_bytes);\n",
        "        cudaMemset(d_histogram, 0, hist_bytes);\n",
        "\n",
        "        GPU_Stopwatch s;\n",
        "        simulate <<< blocks, threads >>> (split_workload, d_state, d_next_state, d_histogram, size);\n",
        "        elapsed_time[t] = s.stop();\n",
        "        \n",
        "        printf(\"it:%llu\\t\\tsplit_wload:%d\\t\\ttime:%f\\n\", iterations, split_workload, elapsed_time[t]);\n",
        "        iterations *= 10;\n",
        "    }\n",
        "\n",
        "    // Host variables\n",
        "    int *thread_histograms, *h_histogram;\n",
        "    thread_histograms = (int*) malloc(hist_bytes);\n",
        "    h_histogram = (int*) malloc(N*sizeof(int));\n",
        "    \n",
        "    memset(h_histogram, 0, N*sizeof(int));\n",
        "    cudaMemcpy(thread_histograms, d_histogram, hist_bytes, cudaMemcpyDeviceToHost);\n",
        "    reduce_histogram(h_histogram, thread_histograms, size, N);\n",
        "    \n",
        "    write_csv(\"GPU_naive.csv\", elapsed_time, MAX_POWER10);\n",
        "    print_histogram(h_histogram, N);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing GPU_naive.cu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZG_MrsCcaIOu"
      },
      "source": [
        "### GPU Sate Array  and Shared Memory\n",
        "\n",
        "Implementação semelhante ao *baseline*, com a adição de:\n",
        "\n",
        "- Memória compartilhada para manipulação dos estados \n",
        "- Utilização de registradores para armazenamento e cálculo das frequências do histograma.\n",
        "- Diretiva `atomicAdd();` para redução do histograma\n",
        "- Utilização reduzida da *global memory*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4TRaKLmNLS6c",
        "outputId": "10658868-ac3e-4a2a-d7a2-61f975eef42e"
      },
      "source": [
        "%%writefile GPU_shared_reg.cu\n",
        "\n",
        "#include <string.h>\n",
        "#include <stdbool.h>\n",
        "#include <stdio.h>\n",
        "\n",
        "#include \"initializer.h\"\n",
        "#include \"profiler.h\"\n",
        "#include \"curand.h\"\n",
        "\n",
        "#define THREADS 128\n",
        "// Talvez seja válido variar com o numero de iterações\n",
        "#define BLOCKS (N+THREADS-1)/THREADS\n",
        "#define SHARED_SIZE THREADS*BLOCKS*N\n",
        "\n",
        "__global__\n",
        "void simulate(int iterations, bool *state, int *histogram, size_t size) {\n",
        "\n",
        "    int tId = threadIdx.x + blockDim.x*blockIdx.x;\n",
        "    if(tId >= size-N) return;\n",
        "\n",
        "    __shared__ bool shared_state[SHARED_SIZE];\n",
        "    __shared__ bool temp_state[SHARED_SIZE];\n",
        "\n",
        "    // Displaces each thread to its respective portion of the array\n",
        "    int offset = tId*N;\n",
        "\n",
        "    #include \"registers.h\"\n",
        "\n",
        "    // Copying initial state\n",
        "    for(int i=0; i<N; i++) \n",
        "        shared_state[offset+i] = state[offset+i];\n",
        "    \n",
        "    for(int i=0; i<iterations; i++) {\n",
        "\n",
        "        #include \"equations.h\"\n",
        "\n",
        "        // Updating network for the next iteration\n",
        "        #pragma unroll\n",
        "        for(int j=0; j<N; j++) \n",
        "            shared_state[offset+j] = temp_state[offset+j];\n",
        "        \n",
        "        #include \"registers_sum.h\"\n",
        "        \n",
        "    }\n",
        "\n",
        "    #include \"registers_reduction.h\"\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "__global__\n",
        "void generate_starting_states(bool *states, float *rand, size_t size) {\n",
        "\n",
        "    int tId = threadIdx.x + blockIdx.x*blockDim.x;\n",
        "\n",
        "    // size-N accounts for the last tId iterating N times until the last position\n",
        "    if(tId >= size-N) return;\n",
        "\n",
        "    int offset = tId*N;\n",
        "    #pragma unroll\n",
        "    for(int i=0; i<N; i++)\n",
        "        states[offset+i] = rand[offset+i]>=0.5?true:false;\n",
        "\n",
        "}\n",
        "\n",
        "int main() {\n",
        "\t\n",
        "    \n",
        "    // Kernel variables\n",
        "    int threads = THREADS; \n",
        "    int blocks = BLOCKS;\n",
        "    printf(\"N:%d\\tthreads:%d\\tblocks:%d\\n\", N, threads, blocks);\n",
        "    printf(\"Estimated shared memory needed: %.2fKB\\tNvidia max: 64KB\\n\\n\", (threads*blocks*N)/1E3);\n",
        "\n",
        "    // Default shared is 48KB, this forces more allocation for shared and less for L1\n",
        "    cudaFuncSetCacheConfig(simulate, cudaFuncCachePreferShared);\n",
        "\n",
        "    size_t size = N*threads*blocks;\n",
        "    size_t n_bytes = sizeof(bool) * size;\n",
        "    size_t hist_bytes = sizeof(int) * N;\n",
        "    \n",
        "    // Randomizing starting positions\n",
        "    float *d_rand;\n",
        "    cudaMalloc((void **) &d_rand, size*sizeof(float));\n",
        "    \n",
        "    curandGenerator_t gen;\n",
        "    curandCreateGenerator(&gen, CURAND_RNG_PSEUDO_DEFAULT);\n",
        "    curandSetPseudoRandomGeneratorSeed(gen, 1234ULL);\n",
        "    curandGenerateUniform(gen, d_rand, size);\n",
        "    \n",
        "    // Device network and histogram variables\n",
        "    bool *d_state;\n",
        "    int *d_histogram;\n",
        "    cudaMalloc((void**) &d_state, n_bytes);\n",
        "    cudaMalloc((void**) &d_histogram, hist_bytes);\n",
        "\n",
        "    generate_starting_states<<<blocks,threads>>>(d_state, d_rand, size);\n",
        "\n",
        "    float elapsed_time[MAX_POWER10];\n",
        "    unsigned long long iterations = 10;\n",
        "    for(int t=0; t<MAX_POWER10; t++) {\n",
        "        \n",
        "        int split_workload = iterations / (blocks*threads) + 1;\n",
        "\n",
        "        cudaMemset(d_histogram, 0, hist_bytes);\n",
        "\n",
        "        GPU_Stopwatch s;\n",
        "        simulate <<< blocks, threads >>> (split_workload, d_state, d_histogram, size);\n",
        "        elapsed_time[t] = s.stop();\n",
        "        \n",
        "        printf(\"it:%llu\\t\\tsplit_wload:%d\\t\\ttime:%f\\n\", iterations, split_workload, elapsed_time[t]);\n",
        "        iterations *= 10;\n",
        "    }\n",
        "    \n",
        "    // Host variables\n",
        "    int *h_histogram;\n",
        "    h_histogram = (int*) malloc(hist_bytes);\n",
        "    cudaMemcpy(h_histogram, d_histogram, hist_bytes, cudaMemcpyDeviceToHost);\n",
        "    \n",
        "    write_csv(\"GPU_shared_reg.csv\",  elapsed_time, MAX_POWER10);\n",
        "    print_histogram(h_histogram, N);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing GPU_shared_reg.cu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XT1ip30Io7xu"
      },
      "source": [
        " # Analysing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJ1V5G4uwb6O"
      },
      "source": [
        "%%bash\n",
        "for file in /content/research/boolean-networks/networks/40-100/*/*.txt\n",
        "do\n",
        "    echo $file\n",
        "    python3 equation_to_code.py \"$file\"\n",
        "    nvcc GPU_shared_reg.cu -lcurand -o GPU_shared_reg && ./GPU_shared_reg\n",
        "    nvcc GPU_naive.cu -lcurand -o GPU_naive && ./GPU_naive\n",
        "done"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scDxuM36pMLh"
      },
      "source": [
        "!nvcc -arch=sm_75 -Xptxas -v -lcurand GPU_shared_reg.cu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbA8ihEppei9"
      },
      "source": [
        "# Fetching data from GPU and CPU benchmarks\n",
        "cpu = []\n",
        "gpu_naive= []\n",
        "gpu_shared_reg= []\n",
        "\n",
        "strategy = {'CPU.csv':[], 'GPU_naive.csv':[], 'GPU_shared_reg.csv':[]}\n",
        "\n",
        "for key in strategy:\n",
        "    try:\n",
        "        with open(key, \"r\") as file:\n",
        "            # i controls how many measurements to skip\n",
        "            i = 0\n",
        "            for value in file.readline().split(\",\"):\n",
        "                if i<4:\n",
        "                    i += 1\n",
        "                    continue  \n",
        "                strategy[key].append(float(value)/1000)\n",
        "    except OSError:\n",
        "        print('Could not open/read strategy ' + key + '. Skipping it...')\n",
        "        continue"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96YruK-dPyXz"
      },
      "source": [
        "## Results \n",
        "No gráfico abaixo, podemos notar que a estratégia que utilizou array de estados pré-compilado em conjunto com memória compartilhada foi a que obteve melhor desempenho, seguida dos ponteiros de função, da implementação *naive* em GPU e da implementação da simulação em CPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pt605jEHpx8d"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Number of elements utilized\n",
        "labels = [\"1E5\", \"1E6\", \"1E7\", \"1E8\", \"1E9\"]\n",
        "param = {'CPU.csv':('go-','CPU'), 'GPU_naive.csv':('bo-','GPU Naive'), 'GPU_shared_reg.csv':('ro-','GPU Shared+Registers'), 3:('mo-','Unknown')}\n",
        "\n",
        "# Graph plotting settings\n",
        "plt.figure(figsize=(16,8))\n",
        "for key in strategy:\n",
        "    try:\n",
        "        plt.plot(labels, strategy[key], param[key][0], linewidth=2, label=param[key][1])\n",
        "    except (KeyError, ValueError):\n",
        "        print('Skipping ' + key)\n",
        "        continue\n",
        "\n",
        "plt.xlabel(\"# de iterações em potência de 10\")\n",
        "plt.ylabel(\"Runtime em Segundos\")\n",
        "plt.title(\"CPU vs. GPU Gene Simulation Runtime\")\n",
        "plt.legend(loc=\"upper left\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMEfScu2RDBA"
      },
      "source": [
        "Abaixo segue a tabela de *speedup* aproximado do código comparado à CPU (A FAZER)\n",
        "\n",
        "GPU Strategy | Speedup\n",
        "-------------|---------\n",
        "Naive        |     2x\n",
        "Global Memory| 12x\n",
        "Shared+Registers  |     45x\n",
        "\n",
        "\n",
        "Observando a tabela acima, mesmo ao considerar a estratégia mais simples, fica clara a contribuição da utilização de GPUs para a Simulação de Redes Booleanas. Ainda que o resultado seja satisfatório, outras otimizações como a representação da rede em bits, *profiling* usando *nsight* e refatoração de código podem ser realizadas na presente implementação com intuito de alcançar ainda mais performance. Por fim, para trabalhos futuros é necessário tornar a geração de código pré-compilado simples e automática, de modo que esta ferramenta possa ser utilizada por pesquisadores da área sem necessidade de conhecimentos específicos."
      ]
    }
  ]
}