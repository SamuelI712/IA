{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGxUZx4pW0lh"
      },
      "source": [
        "<h1>Aula 12 - Regularização L1 e L2 na Regressão Logística</h1>\n",
        "\n",
        "O programa abaixo implementa uma Regressão Logística com a função de ativação Softmax. Repare que no algoritmo do gradiente descendente a atualiação é feita de acordo com uma chamada para uma função de regularização:\n",
        "\n",
        "self._regularizer.reg(self._W, alpha, self._m)\n",
        "\n",
        "Nessa aula você irá implementar a regularização L1 e L2, com o objetivo de ver o comportamento dos pesos resultantes do modelo com os dois esquemas de regularização. Para tal, basta implementar o método 'reg' das classes L1 e L2 no código abaixo. \n",
        "\n",
        "Para relembrar, a atualização do gradiente com regularização L1 é o seguinte:\n",
        "\\begin{equation}\n",
        "w_i \\gets w_i - \\alpha \\frac{\\lambda sign(w_i)}{m} - \\alpha\\frac{1}{m} \\bigg(\\sum_{x,y} (h_{\\theta}(x) - y) x_i \\bigg)\n",
        "\\end{equation}\n",
        "\n",
        "E a atualização L2:\n",
        "\\begin{equation}\n",
        "w_i \\gets w_i \\bigg(1 - \\alpha \\frac{\\lambda}{m} \\bigg) - \\alpha \\frac{1}{m} \\bigg( \\sum_{x, y} (h_{\\theta}(x) - y) x_i \\bigg ) \\,,\n",
        "\\end{equation}\n",
        "\n",
        "onde $m$ é o tamanho do conjunto de treinamento, $\\lambda$ é o parâmetro da regularização que indica a força de regularização e $\\alpha$ é a taxa de aprendizagem utilizada no gradiente descendente. A função $sign(x)$ é a função sinal, que retorna $-1$ se $x < 0$, 0 se $x = 0$ e $+1$ se $x > 0$. A biblioteca Numpy oferece uma implementação dessa função, no código abaixo: np.sign(x).\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13HhyvqdW0lu"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhbsJJB-W0mI"
      },
      "source": [
        "class Regularizer:\n",
        "    def __init__(self, lamb):\n",
        "        self._lamb = lamb\n",
        "    \n",
        "    def reg(self, W, alpha, m):\n",
        "        raise NotImplementedError\n",
        "        \n",
        "class L1(Regularizer):\n",
        "    def reg(self, W, alpha, m):\n",
        "        #Implemente aqui a regularização L1\n",
        "        \n",
        "        # W=W-alpha*(W)/m -alpha*(1/m)\n",
        "         W = W - (alpha*W/m - alpha*1/W)# ( (E * self.derivative_sigmoid(A)).dot(self._x_train.T) ))\n",
        "\n",
        "class L2(Regularizer):\n",
        "    def reg(self, W, alpha, m):\n",
        "        \n",
        "        #Implemente aqui a regularização L2\n",
        "       # W=W*(1 -alpha*self._lamb/m)-alpha*(1/m)\n",
        "         W=W*(1-alpha*lamb/m)-alpha*(1/m)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXNFzMbdW0mZ"
      },
      "source": [
        "class LogisticRegressionSoftmax:\n",
        "    \n",
        "    def __init__(self, x_train, y_train, x_test, y_test, regularizer):\n",
        "        \"\"\"\n",
        "        Constructor assumes a x_train matrix in which each column contains an instance.\n",
        "        Vector y_train contains one integer for each instance, indicating the instance's label. \n",
        "        \n",
        "        Constructor initializes the weights W and B, alpha, and a one-vector Y containing the labels\n",
        "        of the training set. Here we assume there are 10 labels in the dataset. \n",
        "        \"\"\"\n",
        "        self._x_train = x_train\n",
        "        self._y_train = y_train\n",
        "        self._x_test = x_test\n",
        "        self._y_test = y_test\n",
        "        self._m = x_train.shape[1]\n",
        "        self._regularizer = regularizer\n",
        "                \n",
        "        self._W = np.random.randn(10, 784) * 0.01\n",
        "        self._B = np.zeros((10, 1))\n",
        "        self._Y = np.zeros((10, m))\n",
        "\n",
        "        for index, value in enumerate(labels):\n",
        "            self._Y[value][index] = 1\n",
        "            \n",
        "    def plot_digit(self, digit):\n",
        "        plt.imshow(self._W[digit, :].reshape(28, 28), cmap='gray')\n",
        "        plt.show()\n",
        "\n",
        "    def softmax(self, Z):\n",
        "        \"\"\"\n",
        "        Computes the softmax value for all values in vector Z\n",
        "        \"\"\"\n",
        "        Z = Z - np.max(Z)\n",
        "        temp = np.exp(Z)\n",
        "        return temp / np.sum(temp, axis=0)\n",
        "\n",
        "    def h_theta(self, X):\n",
        "        \"\"\"\n",
        "        Computes the value of the hypothesis according to the logistic regression rule\n",
        "        \"\"\"\n",
        "        Z = self._W.dot(X) + self._B\n",
        "        return self.softmax(Z)     \n",
        "    \n",
        "    def train(self, iterations, alpha):\n",
        "        \"\"\"\n",
        "        Performs a number of iterations of gradient descend equals to the parameter 'iterations' passed as input,\n",
        "        with a learning rate of 'alpha'.\n",
        "        \"\"\"\n",
        "        \n",
        "        for i in range(iterations):\n",
        "            A = self.h_theta(self._x_train)\n",
        "\n",
        "            pure_error = (A - self._Y)\n",
        "            self._W = self._regularizer.reg(self._W, alpha, self._m) - alpha * pure_error.dot(self._x_train.T) / self._m\n",
        "            self._B = self._B - alpha * np.sum(pure_error, axis=1, keepdims=True) / self._m\n",
        "\n",
        "            if i % 100 == 0:\n",
        "                classified_correctly = np.sum(np.argmax(A, axis=0) == np.argmax(self._Y, axis=0))\n",
        "                Y_hat_test = self.h_theta(images_test)\n",
        "                test_correct = np.count_nonzero(np.argmax(Y_hat_test, axis=0) == self._y_test)\n",
        "                \n",
        "                print('Train: %.2f \\t Validation: %.2f' % ((classified_correctly / m) * 100, (test_correct)/len(self._y_test) * 100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96mgdswKW0m6"
      },
      "source": [
        "<h2>Experimento 1: Regularização L1</h2>\n",
        "\n",
        "O trecho de código abaixo irá executar o treinamento por 1500 iterações do gradiente descendente com $\\alpha = 0.01$ sem regularização e com regularização L1 ($\\lambda = 0.01$). Em seguida serão impressos na tela as imagens dos pesos da Regressão Logística correspondentes a cada um dos dígitos. \n",
        "\n",
        "Você consegue explicar os valores de acurácia e as imagens de pesos impressas na tela? O que acontece quando utilizamos regularização L1? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwLKSMy8W0m-"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "m = 10000\n",
        "images, labels = (x_train[0:m].reshape(m, 28*28) / 255, y_train[0:m])\n",
        "images = images.T\n",
        "\n",
        "images_test = x_test.reshape(x_test.shape[0], 28*28) / 255\n",
        "images_test = images_test.T\n",
        "\n",
        "print('Without Regularization')\n",
        "log_reg_softmax = LogisticRegressionSoftmax(images, labels, images_test, y_test, L1(0))\n",
        "log_reg_softmax.train(1500, 0.01)\n",
        "for i in range(0, 10):\n",
        "    log_reg_softmax.plot_digit(i)\n",
        "\n",
        "print()\n",
        "print('With Regularization L1')\n",
        "log_reg_softmax = LogisticRegressionSoftmax(images, labels, images_test, y_test, L1(0.01))\n",
        "log_reg_softmax.train(1500, 0.01)\n",
        "for i in range(0, 10):\n",
        "    log_reg_softmax.plot_digit(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nf1i8uimW0nM"
      },
      "source": [
        "<h2>Experimento 2: Regularização L2</h2>\n",
        "\n",
        "O trecho de código abaixo irá executar o treinamento por 1500 iterações do gradiente descendente com $\\alpha = 0.01$ com regularização L2 ($\\lambda = 0.01$). Em seguinda serão impressos na tela as imagens dos pesos da Regressão Logística correspondentes a cada um dos dígitos. \n",
        "\n",
        "Como você explica as diferenças entre as imagens de pesos do modelo com L1 e com L2? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDoimHQpW0nS"
      },
      "source": [
        "log_reg_softmax = LogisticRegressionSoftmax(images, labels, images_test, y_test, L2(0.01))\n",
        "log_reg_softmax.train(1500, 0.01)\n",
        "\n",
        "print('With Regularization L2')\n",
        "for i in range(0, 10):\n",
        "    log_reg_softmax.plot_digit(i)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}