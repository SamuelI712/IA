{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTS4_paGqktQ"
      },
      "source": [
        "<h1>Redes Convolucionais</h1>\n",
        "\n",
        "Neste trabalho você irá comparar uma RNA totalmente conectada com uma RNA com uma camada convolucional no problema de reconhecimento de dígitos MNIST. \n",
        "\n",
        "No trecho de código abaixo nós temos a implementação de uma RNA totalmente conectada utilizando a tangente hiperbólica como função de ativação dos neurônios da camada escondida e a função softmax na camada de saída. \n",
        "\n",
        "Como vimos em sala de aula, a arquitetura da RNA nesta implementação é ajustável através do arranjo <i>dims = [784, 100, 10]</i>. Nesse exemplo, a RNA possui 784 entradas (uma para cada pixel de uma imagem MNIST), 100 neurônios na camada escondida e 10 neurônios na camada de saída. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXx8iA-wqkta"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "class RNA:\n",
        "    def __init__(self, dims):\n",
        "        self.dims = dims\n",
        "        self.weights = {}\n",
        "        self.L = len(dims)\n",
        "\n",
        "        for i in range(1, len(dims)):\n",
        "            self.weights[f'W{i-1}'] = np.random.randn(dims[i], dims[i-1]) * (1/np.sqrt(dims[i-1]))\n",
        "            self.weights[f'B{i-1}'] = np.zeros((dims[i], 1))\n",
        "                        \n",
        "    def derivative(self, A):\n",
        "        return 1 - (A ** 2)\n",
        "    \n",
        "    def activation_function(self, Z):\n",
        "        return np.tanh(Z)\n",
        "    \n",
        "    def softmax(self, x):\n",
        "        x = x - np.max(x)\n",
        "        temp = np.exp(x)\n",
        "        return temp / np.sum(temp, axis=0, keepdims=True)\n",
        "\n",
        "    def forward(self, X, Y=None):\n",
        "        self.cache = {}\n",
        "        self.cache['A0'] = X\n",
        "        m = X.shape[1]\n",
        "        for j in range(1, self.L):\n",
        "            if j < self.L - 1:\n",
        "                self.cache[f'Z{j}'] = np.dot(self.weights[f'W{j-1}'], self.cache[f'A{j-1}']) + self.weights[f'B{j-1}']\n",
        "                self.cache[f'A{j}'] = self.activation_function(self.cache[f'Z{j}'])\n",
        "            else:\n",
        "                self.cache[f'Z{j}'] = np.dot(self.weights[f'W{j-1}'], self.cache[f'A{j-1}']) + self.weights[f'B{j-1}']\n",
        "                self.cache[f'A{j}'] = self.softmax(self.cache[f'Z{j}'])\n",
        "        return self.cache[f'A{self.L-1}']\n",
        "    \n",
        "    def backward(self, Y):\n",
        "        self.cache[f'd{self.L-1}'] = self.cache[f'A{self.L-1}'] - Y\n",
        "        self.cache[f'd{self.L-1}'] = np.multiply(self.cache[f'd{self.L-1}'], self.derivative(self.cache[f'A{self.L-1}']))\n",
        "        \n",
        "        for j in reversed(range(0, self.L-1)):\n",
        "            if j > 0:\n",
        "                self.cache[f'd{j}'] = np.dot(self.weights[f'W{j}'].T, self.cache[f'd{j+1}'])\n",
        "                self.cache[f'd{j}'] = np.multiply(self.cache[f'd{j}'], self.derivative(self.cache[f'A{j}']))\n",
        "\n",
        "            self.cache[f'dW{j}'] = (1/len(Y)) * (np.dot(self.cache[f'd{j+1}'], self.cache[f'A{j}'].T))\n",
        "            self.cache[f'dB{j}'] = (1/len(Y)) * np.sum(self.cache[f'd{j+1}'], axis=1, keepdims=True)\n",
        "    \n",
        "    def update(self, alpha):\n",
        "        for j in range(0, self.L-1):\n",
        "            self.weights[f'W{j}'] = self.weights[f'W{j}'] - alpha * self.cache[f'dW{j}']\n",
        "            self.weights[f'B{j}'] = self.weights[f'B{j}'] - alpha * self.cache[f'dB{j}']\n",
        "                    \n",
        "    def train(self, X, Y, X_test, Y_test, alpha, steps):\n",
        "        percentage_train_list = []\n",
        "        percentage_test_list = []\n",
        "        \n",
        "        Y_one_hot = np.zeros((10, X.shape[1]))        \n",
        "        for index, value in enumerate(Y):\n",
        "            Y_one_hot[value][index] = 1\n",
        "        \n",
        "        for i in range(0, steps):\n",
        "            self.forward(X, Y_one_hot)\n",
        "            self.backward(Y_one_hot)\n",
        "            self.update(alpha)\n",
        "            if i % 100 == 0:\n",
        "                percentage_train = self.evaluate(X, Y)\n",
        "                percentage_train_list.append(percentage_train)\n",
        "                \n",
        "                percentage_test = self.evaluate(X_test, Y_test)\n",
        "                percentage_test_list.append(percentage_test)                \n",
        "                \n",
        "                print('train: %.3f, test: %.3f ' % (percentage_train, percentage_test))\n",
        "\n",
        "    def evaluate(self, X, Y):\n",
        "        Y_hat = self.predict(X)\n",
        "        classified_correctly = test_correct = np.count_nonzero(np.argmax(Y_hat, axis=0) == Y)\n",
        "        return classified_correctly / X.shape[1]\n",
        "    \n",
        "    def predict(self, X):\n",
        "        return self.forward(X) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvp-vmCrqktu"
      },
      "source": [
        "Ao executar o código abaixo você irá treinar uma RNA com a estrutura 784, 100, 10 com 10000 instâncias de treinamento. Em cada iteração do gradiente descendente será exibido na tela a porcentagem de instâncias classificadas corretamente no conjunto de treinamento e no conjunto de teste. Nesse experimento utilizamos a taxa de aprendizagem de 0.0001 em 1000 iterações do gradiente descendente.  \n",
        "\n",
        "Logo abaixo desse trecho de código você irá observer os valores de uma execução desse processo de treinamento. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hotDOaQqkt1"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "m = 10000\n",
        "images, labels = (x_train[0:m].reshape(m, 28*28) / 255, y_train[0:m])\n",
        "images = images.T\n",
        "\n",
        "images_test, labels_test = (x_test.reshape(x_test.shape[0], 28*28) / 255, y_test) \n",
        "images_test = images_test.T\n",
        "\n",
        "dims = [784, 100, 10]\n",
        "rna = RNA(dims)\n",
        "rna.train(images, labels, images_test, labels_test, 0.0001, 1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxtSIrgIqkuB"
      },
      "source": [
        "Abaixo você deverá completar o código para implementar uma RNA com uma camada convolucional, conforme vimos na aula 17 da disciplina (ver notas de aulas no PVANet). Isto é, a RNA terá uma camada de convolução com 10 filtros de tamanho 5x5; a função de ativação da camada de convolução será a tangente hiperbólica. A RNA terá mais uma camada de saída totalmente conectada implementando a função softmax. Diferente do código acima, que implementa o algoritmo de gradiente descendente, abaixo você irá implementar o gradiente descentende com mini-batches de tamanho 32. \n",
        "\n",
        "Não é necessário implementar uma classe genérica para a RNA abaixo, basta completar o código de forma a treinar o modelo e obter uma saída parecida com a que vimos em sala de aula.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wo3Rx7UoqkuG"
      },
      "source": [
        "import numpy as np, sys\n",
        "from keras.datasets import mnist\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_digit(image):\n",
        "    \"\"\"\n",
        "    Função para plotar uma imagem, utilizada abaixo para mostrar os filtros aprendidos. \n",
        "    \"\"\"\n",
        "    plt.imshow(image, cmap='gray')\n",
        "    plt.show()\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "images, labels = (x_train[0:10000].reshape(10000, 28*28) / 255,\n",
        "                  y_train[0:10000])\n",
        "\n",
        "\n",
        "one_hot_labels = np.zeros((len(labels),10))\n",
        "for i,l in enumerate(labels):\n",
        "    one_hot_labels[i][l] = 1\n",
        "labels = one_hot_labels\n",
        "\n",
        "test_images = x_test.reshape(len(x_test),28*28) / 255\n",
        "test_labels = np.zeros((len(y_test),10))\n",
        "for i,l in enumerate(y_test):\n",
        "    test_labels[i][l] = 1\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def tanh2deriv(output):\n",
        "    return 1 - (output ** 2)\n",
        "\n",
        "def softmax(x):\n",
        "    x = x - np.max(x)\n",
        "    temp = np.exp(x)\n",
        "    return temp / np.sum(temp, axis=1, keepdims=True)\n",
        "\n",
        "def get_image_section(layer, row_from, row_to, col_from, col_to):\n",
        "    section = layer[:,row_from:row_to, col_from:col_to]\n",
        "    return section.reshape(-1,1,row_to-row_from, col_to-col_from)\n",
        "\n",
        "alpha, iterations = (0.015, 50)\n",
        "pixels_per_image, num_labels = (784, 10)\n",
        "batch_size = 32\n",
        "\n",
        "input_rows = 28\n",
        "input_cols = 28\n",
        "\n",
        "kernel_rows = 5\n",
        "kernel_cols = 5\n",
        "num_kernels = 10\n",
        "\n",
        "hidden_size = ((input_rows - kernel_rows + 1) * \n",
        "               (input_cols - kernel_cols + 1)) * num_kernels\n",
        "\n",
        "kernels = (1/np.sqrt(kernel_rows*kernel_cols))*np.random.randn(kernel_rows*kernel_cols,\n",
        "                                 num_kernels) \n",
        "b_k = np.zeros((1, num_kernels))\n",
        "weights_1_2 = (1/np.sqrt(hidden_size))*np.random.randn(hidden_size, num_labels) \n",
        "b_1_2 = np.zeros((1, num_labels))\n",
        "\n",
        "#faremos iterations + 1 operações de atualização do gradiente descendente com o mini-batch\n",
        "for j in range(iterations + 1):\n",
        "\n",
        "    #variável para somarmos o total de instâncias classificadas corretamente em cada batch\n",
        "    correct_cnt = 0\n",
        "    for i in range(int(len(images) / batch_size)):\n",
        "        batch_start, batch_end=((i * batch_size),((i+1)*batch_size))\n",
        "        layer_0 = images[batch_start:batch_end]\n",
        "        layer_0 = layer_0.reshape(layer_0.shape[0], 28, 28)\n",
        "\n",
        "        sects = list()\n",
        "        for row_start in range(layer_0.shape[1] - kernel_rows+1):\n",
        "            for col_start in range(layer_0.shape[2] - kernel_cols+1):\n",
        "                sect = get_image_section(layer_0,\n",
        "                                         row_start,\n",
        "                                         row_start+kernel_rows,\n",
        "                                         col_start,\n",
        "                                         col_start+kernel_cols)\n",
        "                sects.append(sect)\n",
        "\n",
        "        expanded_input = np.concatenate(sects,axis=1)\n",
        "        es = expanded_input.shape\n",
        "        flattened_input = expanded_input.reshape(es[0]*es[1],-1)\n",
        "        \n",
        "        #complete aqui a implementação\n",
        "        if row_start% 100 == 0:  \n",
        "              percentage_train=(correct_cnt/float(len(test_images)))\n",
        "              percentage_test=(correct_cnt/float(len(images)))\n",
        "              print('train: %.3f, test: %.3f ' % (percentage_train, percentage_test))\n",
        "\n",
        "        kernel_output = flattened_input.dot(kernels)\n",
        "        layer_1 = tanh(kernel_output.reshape(es[0],-1))\n",
        "        layer_2 = np.dot(layer_1,weights_1_2)\n",
        "        correct_cnt += int(np.argmax(layer_2) ==\n",
        "                                np.argmax(test_labels[i:i+1]))\n",
        "        \n",
        "       # if i % 100 == 0:  \n",
        "        #      percentage_train=(correct_cnt/float(len(test_images)))\n",
        "         #     percentage_test=(correct_cnt/float(len(images)))\n",
        "          #    print('train: %.3f, test: %.3f ' % (percentage_train, percentage_test))\n",
        "\n",
        "   \n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4yHMG2bRwP7"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "class RNA:\n",
        "    def __init__(self, dims):\n",
        "        self.dims = dims\n",
        "        self.weights = {}\n",
        "        self.L = len(dims)\n",
        "\n",
        "        for i in range(1, len(dims)):\n",
        "            self.weights[f'W{i-1}'] = np.random.randn(dims[i], dims[i-1]) * (1/np.sqrt(dims[i-1]))\n",
        "            self.weights[f'B{i-1}'] = np.zeros((dims[i], 1))\n",
        "                        \n",
        "    def derivative(self, A):\n",
        "        return 1 - (A ** 2)\n",
        "    \n",
        "    def activation_function(self, Z):\n",
        "        return np.tanh(Z)\n",
        "    \n",
        "    def softmax(self, x):\n",
        "        x = x - np.max(x)\n",
        "        temp = np.exp(x)\n",
        "        return temp / np.sum(temp, axis=0, keepdims=True)\n",
        "\n",
        "    def forward(self, X, Y=None):\n",
        "        self.cache = {}\n",
        "        self.cache['A0'] = X\n",
        "        m = X.shape[1]\n",
        "        for j in range(1, self.L):\n",
        "            if j < self.L - 1:\n",
        "                self.cache[f'Z{j}'] = np.dot(self.weights[f'W{j-1}'], self.cache[f'A{j-1}']) + self.weights[f'B{j-1}']\n",
        "                self.cache[f'A{j}'] = self.activation_function(self.cache[f'Z{j}'])\n",
        "            else:\n",
        "                self.cache[f'Z{j}'] = np.dot(self.weights[f'W{j-1}'], self.cache[f'A{j-1}']) + self.weights[f'B{j-1}']\n",
        "                self.cache[f'A{j}'] = self.softmax(self.cache[f'Z{j}'])\n",
        "        return self.cache[f'A{self.L-1}']\n",
        "    \n",
        "    def backward(self, Y):\n",
        "        self.cache[f'd{self.L-1}'] = self.cache[f'A{self.L-1}'] - Y\n",
        "        self.cache[f'd{self.L-1}'] = np.multiply(self.cache[f'd{self.L-1}'], self.derivative(self.cache[f'A{self.L-1}']))\n",
        "        \n",
        "        for j in reversed(range(0, self.L-1)):\n",
        "            if j > 0:\n",
        "                self.cache[f'd{j}'] = np.dot(self.weights[f'W{j}'].T, self.cache[f'd{j+1}'])\n",
        "                self.cache[f'd{j}'] = np.multiply(self.cache[f'd{j}'], self.derivative(self.cache[f'A{j}']))\n",
        "\n",
        "            self.cache[f'dW{j}'] = (1/len(Y)) * (np.dot(self.cache[f'd{j+1}'], self.cache[f'A{j}'].T))\n",
        "            self.cache[f'dB{j}'] = (1/len(Y)) * np.sum(self.cache[f'd{j+1}'], axis=1, keepdims=True)\n",
        "    \n",
        "    def update(self, alpha):\n",
        "        for j in range(0, self.L-1):\n",
        "            self.weights[f'W{j}'] = self.weights[f'W{j}'] - alpha * self.cache[f'dW{j}']\n",
        "            self.weights[f'B{j}'] = self.weights[f'B{j}'] - alpha * self.cache[f'dB{j}']\n",
        "                    \n",
        "    def train(self, X, Y, X_test, Y_test, alpha, steps):\n",
        "        percentage_train_list = []\n",
        "        percentage_test_list = []\n",
        "        \n",
        "        Y_one_hot = np.zeros((10, X.shape[1]))        \n",
        "        for index, value in enumerate(Y):\n",
        "            Y_one_hot[value][index] = 1\n",
        "        \n",
        "        for i in range(0, steps):\n",
        "            self.forward(X, Y_one_hot)\n",
        "            self.backward(Y_one_hot)\n",
        "            self.update(alpha)\n",
        "            if i % 100 == 0:\n",
        "                percentage_train = self.evaluate(X, Y)\n",
        "                percentage_train_list.append(percentage_train)\n",
        "                \n",
        "                percentage_test = self.evaluate(X_test, Y_test)\n",
        "                percentage_test_list.append(percentage_test)                \n",
        "                \n",
        "                print('train: %.3f, test: %.3f ' % (percentage_train, percentage_test))\n",
        "\n",
        "    def evaluate(self, X, Y):\n",
        "        Y_hat = self.predict(X)\n",
        "        classified_correctly = test_correct = np.count_nonzero(np.argmax(Y_hat, axis=0) == Y)\n",
        "        return classified_correctly / X.shape[1]\n",
        "    \n",
        "    def predict(self, X):\n",
        "        return self.forward(X) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2PXe2nCRqVb"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "class RNA:\n",
        "    def __init__(self, dims):\n",
        "        self.dims = dims\n",
        "        self.weights = {}\n",
        "        self.L = len(dims)\n",
        "\n",
        "        for i in range(1, len(dims)):\n",
        "            self.weights[f'W{i-1}'] = np.random.randn(dims[i], dims[i-1]) * (1/np.sqrt(dims[i-1]))\n",
        "            self.weights[f'B{i-1}'] = np.zeros((dims[i], 1))\n",
        "                        \n",
        "    def derivative(self, A):\n",
        "        return 1 - (A ** 2)\n",
        "    \n",
        "    def activation_function(self, Z):\n",
        "        return np.tanh(Z)\n",
        "    \n",
        "    def softmax(self, x):\n",
        "        x = x - np.max(x)\n",
        "        temp = np.exp(x)\n",
        "        return temp / np.sum(temp, axis=0, keepdims=True)\n",
        "\n",
        "    def forward(self, X, Y=None):\n",
        "        self.cache = {}\n",
        "        self.cache['A0'] = X\n",
        "        m = X.shape[1]\n",
        "        for j in range(1, self.L):\n",
        "            if j < self.L - 1:\n",
        "                self.cache[f'Z{j}'] = np.dot(self.weights[f'W{j-1}'], self.cache[f'A{j-1}']) + self.weights[f'B{j-1}']\n",
        "                self.cache[f'A{j}'] = self.activation_function(self.cache[f'Z{j}'])\n",
        "            else:\n",
        "                self.cache[f'Z{j}'] = np.dot(self.weights[f'W{j-1}'], self.cache[f'A{j-1}']) + self.weights[f'B{j-1}']\n",
        "                self.cache[f'A{j}'] = self.softmax(self.cache[f'Z{j}'])\n",
        "        return self.cache[f'A{self.L-1}']\n",
        "    \n",
        "    def backward(self, Y):\n",
        "        self.cache[f'd{self.L-1}'] = self.cache[f'A{self.L-1}'] - Y\n",
        "        self.cache[f'd{self.L-1}'] = np.multiply(self.cache[f'd{self.L-1}'], self.derivative(self.cache[f'A{self.L-1}']))\n",
        "        \n",
        "        for j in reversed(range(0, self.L-1)):\n",
        "            if j > 0:\n",
        "                self.cache[f'd{j}'] = np.dot(self.weights[f'W{j}'].T, self.cache[f'd{j+1}'])\n",
        "                self.cache[f'd{j}'] = np.multiply(self.cache[f'd{j}'], self.derivative(self.cache[f'A{j}']))\n",
        "\n",
        "            self.cache[f'dW{j}'] = (1/len(Y)) * (np.dot(self.cache[f'd{j+1}'], self.cache[f'A{j}'].T))\n",
        "            self.cache[f'dB{j}'] = (1/len(Y)) * np.sum(self.cache[f'd{j+1}'], axis=1, keepdims=True)\n",
        "    \n",
        "    def update(self, alpha):\n",
        "        for j in range(0, self.L-1):\n",
        "            self.weights[f'W{j}'] = self.weights[f'W{j}'] - alpha * self.cache[f'dW{j}']\n",
        "            self.weights[f'B{j}'] = self.weights[f'B{j}'] - alpha * self.cache[f'dB{j}']\n",
        "                    \n",
        "    def train(self, X, Y, X_test, Y_test, alpha, steps):\n",
        "        percentage_train_list = []\n",
        "        percentage_test_list = []\n",
        "        \n",
        "        Y_one_hot = np.zeros((10, X.shape[1]))        \n",
        "        for index, value in enumerate(Y):\n",
        "            Y_one_hot[value][index] = 1\n",
        "        \n",
        "        for i in range(0, steps):\n",
        "            self.forward(X, Y_one_hot)\n",
        "            self.backward(Y_one_hot)\n",
        "            self.update(alpha)\n",
        "            if i % 100 == 0:\n",
        "                percentage_train = self.evaluate(X, Y)\n",
        "                percentage_train_list.append(percentage_train)\n",
        "                \n",
        "                percentage_test = self.evaluate(X_test, Y_test)\n",
        "                percentage_test_list.append(percentage_test)                \n",
        "                \n",
        "                print('train: %.3f, test: %.3f ' % (percentage_train, percentage_test))\n",
        "\n",
        "    def evaluate(self, X, Y):\n",
        "        Y_hat = self.predict(X)\n",
        "        classified_correctly = test_correct = np.count_nonzero(np.argmax(Y_hat, axis=0) == Y)\n",
        "        return classified_correctly / X.shape[1]\n",
        "    \n",
        "    def predict(self, X):\n",
        "        return self.forward(X) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzVsNPEMqkuQ"
      },
      "source": [
        "O trecho de código abaixo irá plotar o filtros treinados pela RNA. O que você observa nos filtros treinados pelo seu modelo? \n",
        "\n",
        "Escreva aqui a sua resposta: Os filtros treinados pelo modelo evidenciam regiões mais acentuadas, possivelmente características aprendidas de uma parte corespondente de um determinado algarismo.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2xEgc8LqkuU"
      },
      "source": [
        "for i in range(kernels.shape[1]):\n",
        "    plot_digit(kernels[:,i].reshape(kernel_rows, kernel_cols))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}